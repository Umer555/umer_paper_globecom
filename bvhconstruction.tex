\section{IAA: Instance-Aware Aggregation}
\label{sec:bvhconstruction}

Interactively displaying large scenes like those in
Figure~\ref{fig:screenshot-teaser} requires reducing the total number
of models passed to a GPU. Such aggregation is needed even in a scene
with many very simple models since GPUs generally support a very limited draw
call budget.
%A bounding volume hierarchy (BVH) can
%optimize both download latency and rendering performance for clients
%by grouping together similar and duplicate objects. Internal nodes of
%the BVH point to simplified aggregates of many objects. Content
%coherence means that scenes often have many nearby copies of similar
%or identical objects. An aggregate object is the sum of all of the
%object's submeshes. Multiple copies of the same model are turned into
%multiple instances of a submesh, which IAS simplifies to a desired
%file size. Optimizing these aggregates for wide-area delivery involves
%constructing a bounding volume hierarchy that considers not only query
%performance (\eg the surface area heuristic), but also object
%similarity.
A BVH can serve this purpose by grouping together objects into a tree whose
internal nodes point to simplified aggregates of many objects. The GPU can then
display the world by rendering aggregates belonging to a cut through the BVH.
Traditional approaches to BVH construction aim to improve ray tracing and BVH
querying performance (\eg using the surface area heuristic). However, 
content coherence means that scenes often have many nearby copies of similar
or identical objects. Within an aggregate, multiple copies of the same model 
can be turned into multiple instances of a submesh, which IAS can then simplify 
to a desired file size. Therefore, to optimize aggregates for wide-area delivery,
BVH construction should consider not only query
performance, but also object similarity.


\begin{figure}
\centering
\subfigure[Bounding surface area.]
{\includegraphics[width=0.18\textwidth]{fig/bvh.pdf}\label{fig:optimize-SAH}}
\subfigure[Aggregate mesh size.]
{\includegraphics[width=0.18\textwidth]{fig/iaa.pdf}\label{fig:optimize-SIMILAR}}
\caption{Comparing BVHs constructed using different optimization criteria.}
\label{fig:bvh_optimization}
\vspace{-14pt}
\end{figure}

\subsection{Quantifying Object Similarity}
\label{sec:similarity}

The BVH quantifies object similarity using Zernike shape descriptors
\cite{zernike} to compare geometry and the Color Structure Descriptor
(CSD) \cite{csd} to compare textures. Zernike descriptors are
invariant to rotation, translation and scale, while CSDs allow us to
efficiently compare textures that have the same color content, but
different color layout.

Users upload their models to a content delivery network (CDN),
which computes and stores the
Zernike descriptor of each model by first voxelizing the
model to a $128^{3}$ grid~\cite{binvox_1,binvox_2}, and then
generating a 20$^{\text{th}}$ order Zernike descriptor of length 121.
The CDN also computes a 32-bin CSD using charted versions of each model's
textures~\cite{icme-sirikata-paper}. These charted textures help deal with the
vagaries of user-generated content by combining
all colors and textures used in a model into a single image, eliminating
unused parts of the texture space, and re-mapping the texture coordinates
to within the texture dimensions. 

\subsection{Incremental BVH Construction}
\label{sec:bvh_incremental}

Figure~\ref{fig:bvh_optimization} shows the tradeoffs in optimizing a
BVH for delivering models over a WAN.  Optimizing solely
for query performance degrades visual quality because it misses
potential data compression gains that instance-aware simplification
can provide.  Optimizing solely for data compression, however, leads
to poor quality scenes as well as poor compression as very distant
objects with different sizes to a viewer have the same simplification
quality. The BVH construction algorithm therefore trades off these two
metrics to maximize quality.

To insert a new object, the BVH starts at the root and recursively
chooses the child which minimizes the metric $D$:
\begin{eqnarray}
\label{eq:bvh_equation}
D = \mu\gamma + (1-\mu)\eta  \\
\gamma = \omega|(T-C_t)| + (1-\omega)|(S-C_s)| \\
\eta = \frac{V_{c}-V_{old}}{V_{max}}
\end{eqnarray} % don't add a newline here...
where $S$ and $T$ are the new object's shape and texture descriptors,
$C_s$ and $C_t$ are the centroid shape and texture descriptors of the
child, $V_{c}$ is the bounding volume of the child with the new object
added, $V_{old}$ is its original bounding volume and $V_{max}$ is a
normalizing term equal to $\max_{c \in children}{V_{c}}$. The
parameters $\mu$ and $\omega$ are between 0 and 1. $\mu$
controls the weight of visual similarity versus bounding volume, while
$\omega$ controls the weight of texture versus shape within visual
similarity.

IAA traverses down the BVH using Equation~\ref{eq:bvh_equation} until
it reaches a leaf node. If the chosen node has space, the algorithm
inserts the object and returns.  If the chosen node is already full,
it splits to accomodate the new object.  The splitting algorithm,
first, calculates the bounding volume waste
(unused volume) $W$ of each pair of children's bounding volumes to find
the worst case waste $W_{max}$.  It then selects the pair of children
which have the highest difference in shape descriptors while remaining
within 80\% of $W_{max}$. These two objects seed the split, becoming
the first objects of the two new leaves.  The algorithm groups
remaining objects across the leaves by minimizing the metric in
Equation~\ref{eq:bvh_equation}.

This leaves the question of what values of $\mu$ and $\omega$ to use.
Increasing $\mu$ reduces download sizes but increases the cost of
BVH querying and occlusion culling due to degraded pruning.  
Increasing $\omega$ makes
textures relatively more important, while decreasing it makes geometry
relatively more important. For the set of scenes we describe in
Section~\ref{sec:deduplication}, we have found setting $\mu=0.8$ and
$\omega=0.2$ reduces download sizes by 34\% with no appreciable effect
on querying or ray tracing performance. 
These values are robust to slight changes as well as
scene. 

%\subsection{Evaluation}
%Evaluations on a set of workloads help to find suitable values for
%$\mu$ and $\omega$ that can lead to a good balance of querying
%performance and grouping similar objects within the BVH. The first
%workload is composed of six unique objects instantiated into a larger
%10,000-object suburb scene. The second workload consists of 236 unique
%objects arranged into a 2362-object island scene. The third workload
%is a city scene generated using CityEngine with 60,000 unique
%objects. Many of these objects are very similar (\eg roads and
%pavements) but are nevertheless distinct. For each workload, the
%generated aggregates remain unsimplified so that their visual quality
%does not change.

%Varying $\mu$ and $\omega$, we compute the average cost of querying
%the BVH. For each $\mu$ and $\omega$, we also compute the average
%amount of data a client has to download on joining the world from ten
%random locations.  Table \ref{tab:mu_querying_effect} shows that the
%average cost of querying the BVH does not increase significantly with
%$\mu$ and $\omega$, except when $\mu$ approaches 1. On the other hand,
%Table \ref{tab:mu_transfer_cost_effect} shows that for two of the
%workloads, the average download size reduces by 29\% to 34\% with
%increasing values of $\mu$. Each of these workloads yields a minimum
%download size at $\mu=0.8$ and $\omega=0.2$. We also found that
%different values of $\omega$ do not have a substantial effect on
%download size.  This suggests that using object geometry to construct
%the BVH provides most of the improvements in download size.  The BVH
%uses $\mu=0.8$ and $\omega=0.2$ as the values of these parameters.

%Note that download size does not decrease in the city scene. All
%objects in the scene are unique. So, while similarity-based
%deduplication facilitates mesh deduplication (Section
%\ref{sec:deduplication}) in this case, it does not directly reduce
%download size.

%{\setlength{\tabcolsep}{.5em}
%\begin{table}
%\centering
%{\small
%\begin{tabular}{lrrrrrr}
%\toprule[2pt]
% & \multicolumn{6}{c}{$\mu$} \\
%\cmidrule{2-7}
% Scene         & 0 & 0.2 & 0.4 & 0.6  & 0.8 & 1.0 \\ \hline
% Suburb     &    167           &  179   & 180   & 192 & 195 & 847 \\
% Island       &     35           &  39 & 41   & 33 & 26  & 49 \\
% City   &     1112            &   1402    & 1459   & 1366 & 1573 & 9240   \\
%\bottomrule[2pt]
%\end{tabular}
%}
%\caption{Effect of $\mu$ on querying cost (average number of visited nodes). 
%All measurements use $\omega$=0.2. Changing
%$\omega$ does not significantly affect querying cost.}

%\label{tab:mu_querying_effect}
%\vspace{-8pt}
%\end{table}
%}

%{\setlength{\tabcolsep}{.5em}
%\begin{table}
%\centering
%{\small
%\begin{tabular}{lrrrrr}
%\toprule[2pt]
% & \multicolumn{5}{c}{$\mu$} \\
%\cmidrule{2-6}
% Scene         & 0       & 0.2    & 0.4    & 0.6   & 0.8  \\ \hline
% Suburb        &   0.564 &  0.431 & 0.454  & 0.433 & 0.397  \\
% Island        &    20.4 &  19.6  & 19.4   & 18.1  & 13.4   \\
% City          &    11.4 &  11.1  & 11.2   & 11.3  & 11.8    \\
%\bottomrule[2pt]
%\end{tabular}
%}
%\caption{Effect of $\mu$ on download cost (in GB). All measurements use $\omega$=0.2. Changing
%$\omega$ does not significantly affect download cost.}

%\label{tab:mu_transfer_cost_effect}
%\vspace{-8pt}
%\end{table}
%}


%\begin{figure}
%\centering
%\includegraphics[width=0.4\textwidth]{fig/tex_zer_querying.pdf}
%\caption{Effect of $\mu$ and $\omega$ on BVH querying performance. 
%}
%\label{fig:tex_zer_querying}
%\vspace{-8pt}
%\end{figure}

%\begin{figure}[t]
%\centering
%\subfigure[Suburb Scene] {
%\includegraphics[width=3.0in]{fig/tex_zer_qualityperbit_suburb.pdf}
%\label{fig:downloadsize_suburb}
%}
%\subfigure[Island Scene]{
%\includegraphics[width=3.0in]{fig/tex_zer_qualityperbit_jterrace.pdf}
%\label{fig:downloadsize_jterrace}
%}
%\subfigure[CityEngine Scene]{
%\includegraphics[width=3.0in]{fig/tex_zer_qualityperbit_cityengine.pdf}
%\label{fig:downloadsize_cityengine}
%}
%\caption{
%Effect of $\mu$ and $\omega$ on average download size for a client.
%}
%\label{fig:downloadsize}
%\end{figure}


%\subsection{Bulk Construction}
%\label{sec:bulkconstruction}

%Every few hours, the system reconstructs the BVH to ensure it does not
%stray too far from the optimal condition.
%A top-down bulk construction algorithm considers a set of axis-aligned
%candidate split planes at each level of the tree.
%It computes the
%mutual similarity, defined as the average distance of the shape
%descriptors from their centroid, of the groups of objects on either side of each plane.
%This computation takes $O(n)$ time, where $n$
%is the number of objects in the system. The partition plane is then chosen as the
%plane that yields the maximum similarity among objects in the two partitions. Thus, the
%whole tree can be reconstructed in $O(n log(n))$ time.

\begin{figure}
\centering
\subfigure[Volume-based aggregation (129MB, $\mu=0$)]{\includegraphics[width=2.5in]{fig/village-volume-129mb-trimmed.png}}
\subfigure[Instance-aware aggregation (120MB. $\mu=0.8$)]{\includegraphics[width=2.5in]{fig/village-iaa-120mb-trimmed.png}}
\caption{Visual improvement IAA provides on the village scene. Both
  scenes are displayed using $\approx$125MB of aggregate and individual meshes.  
  Without IAS, the scene grows to 2.1 GB. Because IAA maintains
  greater instancing, it can fit more visual detail within a limited
  transfer size. Notice in particular the greater tree cover.}
\label{fig:village-iaa}
\vspace{-12pt}
\end{figure}

Figure~\ref{fig:village-iaa} shows the visual quality improvements IAA
provides on the same village scene shown in Figure~\ref{fig:village}.
Unlike in that figure, where the entire scene was a single mesh, these
pictures are from a virtual world client that is displaying the scene
as a hierarchy of aggregates. The scene has 13,523 objects. Houses
have $\approx$1000 triangles and trees have 80, for a total of over 9
million triangles. Instance-aware aggregation causes aggregates to
have fewer unique submeshes, so each submesh needs less
simplification. For a given data transfer size, this results in a
higher quality model.


\subsection{Mesh Deduplication}
\label{sec:deduplication}

The algorithm described above groups visually similar objects
together into larger meshes for instance-aware simplification, such
that copies of the same object can become submesh instances. We can
further improve compression because two submeshes might be different
but visually almost identical. This occurs when there are very similar
objects or when two aggregates joined in a larger aggregate have
simplified the same submesh slightly differently. These submeshes can
be deduplicated by replacing them with instances of a single,
representative submesh.  While deduplicating meshes, it is critical to
ensure that only objects that appear very similar are deduplicated.
This requires finding suitable thresholds for differences in shape and
texture descriptors, below which a pair of models can be considered
nearly identical.

We find these deduplication thresholds by rendering each model in our dataset as
images from three mutually orthogonal directions at varying
sizes. For each pair of
models, the difference in their Zernike descriptors, $z_{diff}$, is
defined as the L2-distance between them, and the difference in texture
descriptors, $t_{diff}$ as the L1-distance. For each such pair,
at each image size, we use a perceptual difference
metric~\cite{perceptual_diff} to determine if their images are
indistinguishable from all three directions. We then note, for each
image size, the minimum values of $z_{diff}$ and $t_{diff}$ below
which all pairs of models are indistinguishable.  This information
efficiently identifies candidates for deduplication with negligible
loss of visual quality. Of course, if it is tolerable to trade-off
visual quality for higher efficiency, these thresholds can be set to
higher values. Finally, we align deduplicated meshes by
using a transformation to line up their major and minor axes.

\subsection{Texture Management and Pipeline Summary}
\label{sec:textures}

\label{sec:generation}

By themselves, the above algorithms can generate models which GPUs
render slowly as they accumulate thousands of textures and
huge overall texture sizes.  To deal with these constraints, the
pipeline uses texture atlasing. The system combines all of an object's
textures into a single atlas and remaps the texture coordinates in the
aggregate mesh to point to the appropriate coordinates in the atlas.
To avoid the complexities of wrapping texture coordinates and
unreferenced parts of textures, we use the charted version of a
model's texture, described in Section~\ref{sec:similarity}.

% Each texture has a maximum size of 128KB.

Some large scenes do not use many unique textures. This is often true
for procedurally generated scenes that reuse only a few textures
many times. To avoid the overhead of
atlasing in this case, we only atlas textures in
aggregates if the scene has more than 100 textures or the
textures occupy over 64MB.

The server generates aggregates for each BVH node by working
bottom-up, merging the meshes of each node's children. The pipeline
for each node works as follows:

\begin{enumerate}
\item Download, triangulate, center and cache the
model for each child from the CDN. 
\item Deduplicate similar models, replacing them 
with multiple instances of a single model (Section~\ref{sec:deduplication}). 
\item Create an aggregate model with the submeshes and
instances from the deduplicated set, using a transformation
matrix that positions and orients the submeshes to match the
corresponding objects in the scene.
\item Apply IAS to the aggregate model (Section~\ref{sec:simplification}).
\item Simplify and atlas the model's textures (Section~\ref{sec:textures}).
\end{enumerate}

We use the time heuristic of Cheslack-Postava \etal to separate
dynamic and static objects into separate BVH structures, only
aggregating static objects~\cite{ewencp2012}.  Objects added to the
world join the dynamic set and are asynchronously incorporated into
the static set. Objects from the static set leaving the world
experience some lag before all of their aggregates update.
A server maintains a client's perspective in the virtual world as
dynamically changing cuts through these BVHs. 
It sends the object and
aggregate identifiers of the elements of the cut to the client (URLs),
which downloads them from the CDN.
